{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5 Transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNECq36bbfr+zbXUGmjAKS/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yunhui21/M_study/blob/main/T5_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeBsypB9HqIc"
      },
      "source": [
        "\n",
        "# The Guide to Multi-Tasking with the T5 Transformer\n",
        "\n",
        "The T5 Transformer can perform any NLP task. It can perform multiple taskes, at the same time, with the same model. Here's how!\n",
        "\n",
        "Thilina Rajapakse\n",
        "\n",
        "The T5(Text-To-Text Transfer Transformer) model was the product of a large-scale study (paper [10683](https://arxiv.org/abs/1910.10683)) conducted to explore the limits of transfer learning. It builds upon popular architectures like GPT, BERT, and RoBERT(to name only a few) models that utilized Transfer Learning with incredible success. While BERT-like models can be fine-tuned to perform a variety of tasks, the constraints of the architecture mean that each model can perform only one task.\n",
        "\n",
        "Typically, this is done by adding a task-specific layer on top of the Transformer model. For example, a Bert Transformer can be adapted for binary classification by adding a fully-connected layer with two output neurons(corresponding to each class). The T5 model departs form this tradition by reframing all NLP tasks as text-to-text tasks. This results in a shared framwork for any NLP task as the input to the model and the output from the model is always a string. In the example of binary classification, the T5 model will simply output a string representation for the class (i.e.\"0\" or \"1\")\n",
        "\n",
        "Since the input and output formats are identical for any NLP task, the same T5 model can be taught to perform multiple tasks! To specify which task shuould be performed, we can simpy prepend a prefix(string) to the input of the model. The animation (shown below)from the Google AI Blog article demondtrates this concept.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/960/0*iK5VVgPA2z_WgvwT.gif\" alt=\"history\" width=''>\n",
        "\n",
        "*From the article Exploring Transfer learning with T5: the Text-To-Text Transfer Transformer*\n",
        "\n",
        "In this article, we'll be using this technique to train a single T5 model capable of performing the 3 NLP tasks, binary classification, multi-label classification, and regression.\n",
        "\n",
        "All code can also be found on [Github.](https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples/t5/mixed_tasks)\n",
        "\n",
        "## Task Specification\n",
        "\n",
        "**Binary Classfication**\n",
        "\n",
        "The goal of binary classificaton in NLP is to classify a given text sequence into one of two classes. In our task, we will be using the Yelp Reviews dataset to classify the sentiment of the text as either positive(\"1\") or negative(\"0\")\n",
        "\n",
        "**Multi-label Classification**\n",
        "In mumti-label classification, a given text sequence should be labeled with the correct subset of a set of pre-defined labels (note that the subset can include both the null set and the full set of labels itself). For this, we will be using the Toxic Comments dataset where each text can be labeled with any subset of the labels toxic, severe_toxic, obscene, thereat, insult, identity_hate.\n",
        "\n",
        "**Regression**\n",
        "In regression tasks, the target variable is a continuous value. In our task, we will use the STS-B(Semantic Textual Similarity Benchmark) dataset where the goal is to predict the similarity of two sentences. The similarity is denoted by a dontinuous value between 0 and 5.\n",
        "\n",
        "##Data Preparation\n",
        "Since we are going to be working with 3 datasets, we'll put them in 3 seperate subdirectories inside the data directory.\n",
        "\n",
        "* data/binary_classification\n",
        "* data/multilabel_classification\n",
        "* data/regression\n",
        "\n",
        "**Downloading**\n",
        "1.Download the [Yelp Reviews Dataset.](https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz)\n",
        "2.Extract train.csv and test.csv to data/binary_classification.\n",
        "3.Download the [Toxic Comments dataset.](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/)\n",
        "4.Extract the csv files to data/multilabel_classification.\n",
        "5.Download the [STS-B dataset.](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark)\n",
        "6.Extract the csv files to data/regression.\n",
        "\n",
        "**Combining the datasets**\n",
        "As mentioned earlier, the inputs and outputs fo a T5 model is always text. A particular task is specified by using a prefix text that lets the model know what is should do with the input.\n",
        "\n",
        "The input data format for a T5 model in Simple Transformers reflects this fact. The input is a Pandas dataframe with the 3 columns-prefix, input_text, and target_text. This makes it quite easy to train the model on multiple tasks as you just need to change the prefix.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9tTFNTWStBS"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy5ZC9U2S0f_"
      },
      "source": [
        "prefix = 'data/binary_classification/'\n",
        "\n",
        "binary_train_df = pd.read_csv(prfix + 'train.csv', header = None)\n",
        "binary_train_df.head()\n",
        "\n",
        "binary_eval_df = pd.read_csv(prefix + 'test.csv', header = None)\n",
        "binary_eval_df.head()\n",
        "\n",
        "bianry_train_df[0] = (binary_tarin_df[0] == 2).astype(int)\n",
        "binary_eval_df[0] = (binary_eval_df[0] == 2).astype(int)\n",
        "\n",
        "binary_train_df = pd.DataFrame({'prefix':['binary clssification' for i in range(len(binary_train_df))],\n",
        "                                'input_text':binary_train_df[1].}str.replace('\\n', ''),\n",
        "                                'target_text':binary_train_df[0].astype('str')})\n",
        "\n",
        "print(binary_train_df.head())\n",
        "\n",
        "binary_eval_df = pd.DataFrame({'prfix': []})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}