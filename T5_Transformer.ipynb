{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5 Transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN/+M5zDjdXZ7aHt6k+XSNz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yunhui21/M_study/blob/main/T5_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeBsypB9HqIc"
      },
      "source": [
        "\n",
        "# The Guide to Multi-Tasking with the T5 Transformer\n",
        "\n",
        "The T5 Transformer can perform any NLP task. It can perform multiple taskes, at the same time, with the same model. Here's how!\n",
        "\n",
        "Thilina Rajapakse\n",
        "\n",
        "The T5(Text-To-Text Transfer Transformer) model was the product of a large-scale study (paper [10683](https://arxiv.org/abs/1910.10683)) conducted to explore the limits of transfer learning. It builds upon popular architectures like GPT, BERT, and RoBERT(to name only a few) models that utilized Transfer Learning with incredible success. While BERT-like models can be fine-tuned to perform a variety of tasks, the constraints of the architecture mean that each model can perform only one task.\n",
        "\n",
        "Typically, this "
      ]
    }
  ]
}